{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronal Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from http://nbviewer.jupyter.org/github/rasbt/pattern_classification/blob/master/machine_learning/scikit-learn/outofcore_modelpersistence.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The IMDb Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train a simple logistic regression model to classify movie reviews from the 50k IMDb review dataset that has been collected by Maas et. al.\n",
    "\n",
    "> AL Maas, RE Daly, PT Pham, D Huang, AY Ng, and C Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin- guistics: Human Language Technologies, pages 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics\n",
    "\n",
    "[Source: http://ai.stanford.edu/~amaas/data/sentiment/]\n",
    "\n",
    "The dataset consists of 50,000 movie reviews from the original \"train\" and \"test\" subdirectories. The class labels are binary (1=positive and 0=negative) and contain 25,000 positive and 25,000 negative movie reviews, respectively.\n",
    "For simplicity, I assembled the reviews in a single CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>OK, lets start with the best. the building. al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The British 'heritage film' industry is out of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I don't even know where to begin on this one. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>Richard Tyler is a little boy who is scared of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I waited long to watch this movie. Also becaus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "49995  OK, lets start with the best. the building. al...          0\n",
       "49996  The British 'heritage film' industry is out of...          0\n",
       "49997  I don't even know where to begin on this one. ...          0\n",
       "49998  Richard Tyler is a little boy who is scared of...          0\n",
       "49999  I waited long to watch this movie. Also becaus...          1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# if you want to download the original file:\n",
    "#df = pd.read_csv('https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/50k_imdb_movie_reviews.csv')\n",
    "# otherwise load local file\n",
    "df = pd.read_csv('shuffled_movie_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us shuffle the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "## uncomment these lines if you have dowloaded the original file:\n",
    "#np.random.seed(0)\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df[['review', 'sentiment']].to_csv('shuffled_movie_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define a simple `tokenizer` that splits the text into individual word tokens. Furthermore, we will use some simple regular expression to remove HTML markup and all non-letter characters but \"emoticons,\" convert the text to lower case, remove stopwords, and apply the Porter stemming algorithm to convert the words into their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) #+ ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it at try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'hoa']\n",
      "2\n",
      "[['test'], ['hoa']]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(tokenizer('This :) is a <a> test!\"hoa\" :-)</br>'))\n",
    "\n",
    "#tamaño de la palabra\n",
    "print(len(tokenizer('This :) is a <a> test!\"hoa\" :-)</br>')))\n",
    "\n",
    "#convertir tokesn a lista de lista\n",
    "\n",
    "def toList(text):\n",
    "    review = []\n",
    "    for w in text:\n",
    "        review.append([w])\n",
    "    return review\n",
    "    \n",
    "print(toList(tokenizer('This :) is a <a> test!\"hoa\" :-)</br>')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning (SciKit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a generator that returns the document body and the corresponding class label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conform that the `stream_docs` function fetches the documents as intended, let us execute the following code snippet before we implement the `get_minibatch` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n",
       " 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='shuffled_movie_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we confirmed that our `stream_docs` functions works, we will now implement a `get_minibatch` function to fetch a specified number (`size`) of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    for _ in range(size):\n",
    "        text, label = next(doc_stream)\n",
    "        \n",
    "        docs.append(toList(tokenizer(text)))\n",
    "        y.append(label)\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primero vamos cargar todos los datos en un X y Y para extraer las caracteristias \n",
    "\n",
    "X,Y = get_minibatch(stream_docs(path='shuffled_movie_data.csv'), size= 50000) # get string of text\n",
    "\n",
    "#print(ar[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraccion de feature  de word2vect con gensim desde Cero\n",
    "\n",
    "en es paso vamos usar la libreria Gensim para generar nuestro proppio diccionarion de un Word2vect embedding\n",
    "de tal forma que tambien podemos definir el tamaño de los vectores por palabra, para este ejemplo y por motivos de prueba solo consideramos 20 el tamaño de cada vector,sin embargo se recommienda de 100-300 para el optimo desempeño, lo que se hace aqui es:\n",
    "- generar nuestro modelo word2vect = n\n",
    "- almacenamos las palabras con fuerte relacion de vencidad en un diccionario = w2v\n",
    "- consideramos el promedio de los vectores para su mejor representacion de los features\n",
    "- finalmente se almacena  en un X_total (total de features 'matriz'50000 x 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X: 50000\n",
      "size of X[4]:  62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewilderd/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "# tamaño se texto X, Y\n",
    "\n",
    "print('size of X:',len(X))\n",
    "print('size of X[4]: ',len(X[4]))\n",
    "#print('dato :',X[0])\n",
    "\n",
    "#definir el numero de  el word embedding para cada review\n",
    "import gensim\n",
    "\n",
    "    \n",
    "#word to vect para un review \n",
    "\n",
    "#recorrer para los 50 de X\n",
    "#X_train = []\n",
    "\n",
    "    \n",
    "#print(X[0])    \n",
    "\n",
    "X_total = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    n = gensim.models.Word2Vec(X[i], size= 20,min_count=1)\n",
    "    w2v = dict(zip(n.wv.index2word, n.wv.syn0))\n",
    "    letra = list(n.wv.vocab)\n",
    "    da = []\n",
    "    for i in range(len(w2v)):\n",
    "        da.append(w2v[letra[i]])\n",
    "    X_total.append(np.mean(da, axis = 0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make use of the \"hashing trick\" through scikit-learns [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) to create a bag-of-words model of our documents. Details of the bag-of-words model for document classification can be found at  [Naive Bayes and Text Classification I - Introduction and Theory](http://arxiv.org/abs/1410.5329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 20), (5000, 20))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "# Exercise 1: define features based on word embeddings (pre-trained word2vec / Glove/Fastext emebddings can be used)\n",
    "# Define suitable d dimension, and sequence length\n",
    "\n",
    "#Definiremos el dato para el test\n",
    "X_train,X_test=X_total[:45000], X_total[45000:]\n",
    "X_test = np.asarray(X_test)\n",
    "X_train = np.asarray(X_train)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptando mis datos para my own NN 3layers\n",
    "\n",
    "en esta parte adaptamos los datos para que se puedan operar de la forma que implemente la red neuronal de 3\n",
    "capas , aqui se puede ver:\n",
    "- x_train, X_test, Y_train, Y_test que son el dato para el training y el test, respectivamente con 45 mil y 5 mil para el test.\n",
    "- tambien debemos decir que X_total , Y_total son la cantidad de features y labels totales 50 mil filas cada uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "penultimo:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((45000, 2), (5000, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import gensim\n",
    "# sea X la lista de textos token (es decir lista de lista de tokens)\n",
    "modelo = gensim.models.Word2Vec(X, size=20, workers= 4)\n",
    "w2v = dict(zip(modelo.wv.index2word, modelo.wv.syn0))\n",
    "# reducir el modelo en vocabulario de tamaño en el modelo\n",
    "words = list(modelo.wv.vocab)\n",
    "#guardaremos el modleo\n",
    "modelo.save('model.bin')\n",
    "#abrir el modelo y cargar\n",
    "new_model = gensim.models.Word2Vec.load('model.bin')\n",
    "\n",
    "print('words: ',len(words))\n",
    "'''\n",
    "\n",
    "X = np.asarray(X_train)\n",
    "#Y = np.asarray(Y)\n",
    "#Y_train = np.zeros((5000,2))\n",
    "y_1 = []\n",
    "y_2 = []\n",
    "#validar Y_train\n",
    "for k in range(len(Y)):\n",
    "    if Y[k] == 1:\n",
    "        y_1.append(Y[k])  \n",
    "        y_2.append(0)\n",
    "    else:\n",
    "        y_1.append(0)  \n",
    "        y_2.append(Y[k])\n",
    "Y_total = np.column_stack((np.array(y_1),np.array(y_2)))\n",
    "print('penultimo: ',Y_total[0][0])\n",
    "\n",
    "#dividir y\n",
    "Y_train = np.asarray(Y_total[:45000])\n",
    "Y_test = np.asarray(Y_total[45000:])\n",
    "        \n",
    "Y_train.shape , Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My own NN 3layes\n",
    "\n",
    "en esta parte se implemente mi propia red neuronal, con tres capas , debemos decir que esta comentado en cada paso para su mejor comprension, y puede ser mejorada por ud, usamos :\n",
    "\n",
    "- fordward propagation: la progacion directa y hallar para los primero z = x*w + b , 1,2,3\n",
    "- backward propagation: se implementó la propagación backward con al gradiente descendiente derivando los errores de costo para ajustar los pesos por convergencia.\n",
    "- tambien se utilizo, solo con fines de aprendizaje la funcion de activacion tanh, ud, puede optar por sigmoide.\n",
    "- y finalmente esto tambien puede adaptarse a mas de 2 salidas pues usa softmax en la ultima capa.\n",
    "- adicionalmente se hace un test del training y su acuraccy y se ploteo tmb para ver el desarrollo por iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss despues de iter:  0 : 0.348393709003807\n",
      "training Accuracy despues de iter:  0 : 0.1711111111111111 %\n",
      "Loss despues de iter:  100 : 0.14338126206093127\n",
      "training Accuracy despues de iter:  100 : 100.0 %\n",
      "Loss despues de iter:  200 : 0.1438039606961593\n",
      "training Accuracy despues de iter:  200 : 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f10ca4e4b00>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Package imports\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score# Aqui vamos a definir todas nuestra funciones \n",
    "# de la Red Neuronal de 3 capas\n",
    "\n",
    "# funcion softmax\n",
    "def softmax(z):\n",
    "    #Calculamos el exponente\n",
    "    # e^z\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # haciendo Clipping a los valores\n",
    "    minval = 0.000000000001\n",
    "    # Numero de muestras \n",
    "    m = y.shape[0]\n",
    "    # formula Loss , con np.sum suma la matriz completa y ademas hace el trabajo\n",
    "    # de dos sumas para la formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "#PROPAGACION FORWARD\n",
    "# Este es la funcion de progracacion Directa\n",
    "def forward_prop(model,a0):\n",
    "\n",
    "    # Cargamos los parametro del modelo\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # hacemos el primer paso lineal\n",
    "    # Z1 es la capa input: x veces el producto punto con weights +  (bias) b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Pasamos luego la funcion de activacion\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Segundo paso lineal\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Segunda funcion de activacion\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Tercer paso lineal\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #Aqui, en la tercera funcion de activacion usamos softmax,puede usarse\n",
    "    #tambien el sigmoide softmax para la ultima capa\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Guardamos todos los resultados en cache\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# Esto es la funcion backward propagation\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Cargamos los parametros del modelo\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Cargamos los resultados de forward propagation\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Sacamos el numero de muestras\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculamos la derivada 'loss' con respeto a la salida\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculamos la derivada loss con respecto a la 2da capa y pesos\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calcular la derivada loss del bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calcular la derivada loss de la 1ra capa\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calcular la derivada loss del 1er peso\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calcular la derivada loss dela primera capa bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Guardar los gradientes\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads\n",
    "\n",
    "#FASE DE ENTRENAMIENTO\n",
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # Primera capa con pesos\n",
    "    # creamos matriz de:13 ,5) random -1 values\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # 1ra capa bias : 1 row y nn_hdim colums\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Segunda capa W\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Segunda capa bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package y return modelo\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3 }\n",
    "    return model\n",
    "\n",
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Cargar parametros\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Actualizar parametros\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Guardas y return parametros\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "\n",
    "def predict(model, x):\n",
    "    # hacer el paso forward\n",
    "    c = forward_prop(model,x)\n",
    "    # Obtener y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Obtener el numero total de muestras\n",
    "    m = y.shape[0]\n",
    "    # Hacer la prediccion para el modelo\n",
    "    pred = predict(model,x)\n",
    "    # Asegurar la prediccion y el vector real y tener la forma\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calcular el numero de muestras malas\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calcular el accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "\n",
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradiente descendiente. bluce de epocas\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradiente descendiente y actualar parametros\n",
    "        # Asignar nuevo parametros al modelo\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Imprimir loss & accuracy cada 100 iteraciones\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss despues de iter: ',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('training Accuracy despues de iter: ',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model\n",
    "\n",
    "#inicializamos la semilla random\n",
    "np.random.seed(0)\n",
    "# main \n",
    "model = initialize_parameters(nn_input_dim=20, nn_hdim= 5, nn_output_dim= 2)\n",
    "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=300,print_loss=True)\n",
    "plt.plot(losses)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test my ow NN\n",
    "\n",
    "en esta parte se hace el test de la red neuronal de 3 capas y se pude ver el acuracy tambien , debemos decir que no se considero directamente, se recomienda usar cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY OWN IMPLEMENTATION TEST ACCURACY: \n",
      "[0 0 0 ... 0 0 0]\n",
      "49.36 %\n"
     ]
    }
   ],
   "source": [
    "print('MY OWN IMPLEMENTATION TEST ACCURACY: ')\n",
    "y_h = predict(model,np.asarray(X_test))\n",
    "print( y_h)\n",
    "print(accuracy_score(y_pred=y_h,y_true=Y[45000:])*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con la red neuronal MLPClassifier  (skLearn)\n",
    "\n",
    "una ves implementado nuestro propia red neuronal ahora vamos probar con la red neuronal del sklearn\n",
    "con la cual tambien haremos el entrenamiento y el test.\n",
    "asi que hechemos las ganas y a probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKLEARN NN accuracy of Test:  49.36 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,20), random_state=1)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=300)\n",
    "classes = np.array([0, 1])\n",
    "clf.partial_fit(X_train, Y[:45000], classes=classes)\n",
    "y_h2 = clf.predict(X_test)\n",
    "\n",
    "\n",
    "print('SKLEARN NN accuracy of Test: ',accuracy_score(y_pred=y_h2,y_true=Y[45000:])*100,'%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [SGDClassifier]() from scikit-learn, we will can instanciate a logistic regression classifier that learns from the documents incrementally using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss='log', random_state=1, max_iter=1)\n",
    "doc_stream = stream_docs(path='shuffled_movie_data.csv')\n",
    "\n",
    "# Exercise 2: Define at least a Three layer neural network. Define its structure (number of hidden neurons, etc)\n",
    "# Define a nonlinear function for hidden layers.\n",
    "# Define a suitable loss function for binary classification\n",
    "# Implement the backpropagation algorithm for this structure\n",
    "# Do not use Keras / Tensorflow /PyTorch etc. libraries\n",
    "# Train the model using SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyprind\n",
    "#pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    #pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your machine, it will take about 2-3 minutes to stream the documents and learn the weights for the logistic regression model to classify \"new\" movie reviews. Executing the preceding code, we used the first 45,000 movie reviews to train the classifier, which means that we have 5,000 reviews left for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.867\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))\n",
    "#Exercise 3: compare  with your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that the predictive performance, an accuracy of ~87%, is quite \"reasonable\" given that we \"only\" used the default parameters and didn't do any hyperparameter optimization. \n",
    "\n",
    "After we estimated the model perfomance, let us use those last 5,000 test samples to update our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we successfully trained a model to predict the sentiment of a movie review. Unfortunately, if we'd close this IPython notebook at this point, we'd have to go through the whole learning process again and again if we'd want to make a prediction on \"new data.\"\n",
    "\n",
    "So, to reuse this model, we could use the [`pickle`](https://docs.python.org/3.5/library/pickle.html) module to \"serialize a Python object structure\". Or even better, we could use the [`joblib`](https://pypi.python.org/pypi/joblib) library, which handles large NumPy arrays more efficiently.\n",
    "\n",
    "To install:\n",
    "conda install -c anaconda joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./clf.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "if not os.path.exists('./pkl_objects'):\n",
    "    os.mkdir('./pkl_objects')\n",
    "    \n",
    "joblib.dump(vect, './vectorizer.pkl')\n",
    "joblib.dump(clf, './clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above, we \"pickled\" the `HashingVectorizer` and the `SGDClassifier` so that we can re-use those objects later. However, `pickle` and `joblib` have a known issue with `pickling` objects or functions from a `__main__` block and we'd get an `AttributeError: Can't get attribute [x] on <module '__main__'>` if we'd unpickle it later. Thus, to pickle the `tokenizer` function, we can write it to a file and import it to get the `namespace` \"right\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tokenizer.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    text = [w for w in text.split() if w not in stop]\n",
    "    tokenized = [porter.stem(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tokenizer.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizer import tokenizer\n",
    "joblib.dump(tokenizer, './tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us restart this IPython notebook and check if the we can load our serialized objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "tokenizer = joblib.load('./tokenizer.pkl')\n",
    "vect = joblib.load('./vectorizer.pkl')\n",
    "clf = joblib.load('./clf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the `tokenizer`, `HashingVectorizer`, and the tranined logistic regression model, we can use it to make predictions on new data, which can be useful, for example, if we'd want to embed our classifier into a web application -- a topic for another IPython notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['I did not like this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['I loved this movie']\n",
    "X = vect.transform(example)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
